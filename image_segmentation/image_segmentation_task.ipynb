{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Alberto Rossettini\n## Explanation of experiments on .txt"},{"metadata":{"id":"0s6Ll73PegaT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":27,"outputs":[]},{"metadata":{"id":"e5OYQ_l0egae","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"c95bfaa6-ded0-408b-925c-b04f1a31bfc3","trusted":true},"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\n\nSEED = 777\ntf.random.set_seed(SEED)  \n\ncwd = os.getcwd()\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","execution_count":28,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Generator\n#### best scoring models without augmentation . By default augmentation set to False"},{"metadata":{"id":"ld8W2mACegap","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# ImageDataGenerator\n# ------------------\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\napply_data_augmentation = False\n\n# Create training ImageDataGenerator object\n# We need two different generators for images and corresponding masks\nif apply_data_augmentation:\n    train_img_data_gen = ImageDataGenerator(rotation_range=5,\n                                            width_shift_range=5,\n                                            height_shift_range=5,\n                                            zoom_range=0.1,\n                                            validation_split=0.2,\n                                            horizontal_flip=True,\n                                            vertical_flip=True,\n                                            fill_mode='reflect',\n                                            rescale=1./255)\n    \n    train_mask_data_gen = ImageDataGenerator(rotation_range=5,\n                                             width_shift_range=5,\n                                             height_shift_range=5,\n                                             zoom_range=0.1,\n                                             validation_split=0.2,\n                                             horizontal_flip=True,\n                                             vertical_flip=True,\n                                             fill_mode='reflect',\n                                             rescale=1./255,\n                                             )\nelse:\n    train_img_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n    train_mask_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Create validation and test ImageDataGenerator objects\nvalid_img_data_gen = ImageDataGenerator(rescale=1./255)\nvalid_mask_data_gen = ImageDataGenerator(rescale=1./255)\ntest_img_data_gen = ImageDataGenerator(rescale=1./255)\ntest_mask_data_gen = ImageDataGenerator(rescale=1./255)","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset:\n### make sure that the dataset is in the current folder, otherwise change dataset_dir to your own."},{"metadata":{"id":"_f0hc7PIegau","colab_type":"code","outputId":"877a3547-b4e3-43d7-e53a-8cc3be2fa50b","colab":{"base_uri":"https://localhost:8080/","height":139},"trusted":true},"cell_type":"code","source":"# Create generators to read images from dataset directory\n# -------------------------------------------------------\ndataset_dir = os.path.join('../input', 'annanddlimagessegmentation/Segmentation_Dataset')\n\n# Batch size\nbs = 16\n\n# img shape\nimg_h = 256\nimg_w = 256\n\nnum_classes=2\n\n# Training\n\ntraining_dir = os.path.join(dataset_dir, 'training')\ntrain_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n                                                       batch_size=bs, \n                                                       class_mode=None, \n                                                       shuffle=True,\n                                                       subset=\"training\",\n                                                       seed=SEED,\n                                                       color_mode='rgb')  \ntrain_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n                                                         batch_size=bs,\n                                                         class_mode=None, \n                                                         shuffle=True,\n                                                         subset=\"training\",\n                                                         seed=SEED,\n                                                         color_mode='grayscale')\ntrain_gen = zip(train_img_gen, train_mask_gen)\n\n# Validation\nvalidation_dir = os.path.join(dataset_dir, 'training')\nvalid_img_gen = train_img_data_gen.flow_from_directory(os.path.join(validation_dir, 'images'),\n                                                       batch_size=bs, \n                                                       class_mode=None, \n                                                       shuffle=False,\n                                                       subset=\"validation\",\n                                                       seed=SEED,color_mode='rgb')\n\nvalid_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(validation_dir, 'masks'),\n                                                         batch_size=bs, \n                                                         class_mode=None, \n                                                         shuffle=False,\n                                                         subset=\"validation\",\n                                                         seed=SEED, color_mode='grayscale')\nvalid_gen = zip(valid_img_gen, valid_mask_gen)\n\n# Test\ntest_dir = os.path.join(dataset_dir, 'test')","execution_count":30,"outputs":[{"output_type":"stream","text":"Found 6118 images belonging to 1 classes.\nFound 6118 images belonging to 1 classes.\nFound 1529 images belonging to 1 classes.\nFound 1529 images belonging to 1 classes.\nFound 1234 images belonging to 1 classes.\n","name":"stdout"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"\"test_mask_gen = test_mask_data_gen.flow_from_directory(os.path.join(test_dir, 'masks'),\\n                                                       target_size=(img_h, img_w),\\n                                                       batch_size=bs, \\n                                                       class_mode=None, # Because we have no class subfolders in this case\\n                                                       shuffle=False,\\n                                                       interpolation='bilinear',\\n                                                       seed=SEED)\""},"metadata":{}}]},{"metadata":{"id":"AKGPxBeZegay","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Create Dataset objects\n# ----------------------\n\n# Training\n# --------\ntrain_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n\ndef prepare_target(x_, y_):\n    y_ = tf.cast(y_, tf.int32)\n    return x_, y_\n\ntrain_dataset = train_dataset.map(prepare_target)\n\n\n# Repeat\ntrain_dataset = train_dataset.repeat()\n\n# Validation\n# ----------\nvalid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\nvalid_dataset = valid_dataset.map(prepare_target)\n\n# Repeat\nvalid_dataset = valid_dataset.repeat()\n","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rsidual blocks to be used in the net"},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = tf.keras.layers.Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n\ndef BatchActivate(x):\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = tf.keras.layers.Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The model: \n"},{"metadata":{"id":"3op_2ZiUega2","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Create Model\n# ------------\ndef create_model_noaug(start_f = 16):\n    start_f = start_f\n    input_shape = [img_h, img_w, 3]\n    inputs = tf.keras.Input((img_h, img_w, 3))\n    \n    # Encoder\n    # -------\n    conv1 = tf.keras.layers.Conv2D(filters=start_f, #256_256_16\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',\n                                         input_shape=input_shape, activation='relu', \n                                         kernel_initializer=\"he_normal\")(inputs)\n    conv1 = residual_block(conv1,start_f)\n    conv1 = residual_block(conv1,start_f, True)                                    \n    pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv1) #128_128_16\n    start_f = start_f*2\n    drop1 = tf.keras.layers.Dropout(0.5*0.05)(pool1)\n    conv2 = tf.keras.layers.Conv2D(filters=start_f,  #128_128_32\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same', activation='relu', \n                                   kernel_initializer=\"he_normal\")(drop1)\n    conv2 = residual_block(conv2,start_f)\n    conv2 = residual_block(conv2,start_f, True)                             \n    pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv2) #64_64_32\n    drop2 = tf.keras.layers.Dropout(0.5)(pool2)\n    start_f = start_f*2\n    conv3 = tf.keras.layers.Conv2D(filters=start_f, #64_64_64\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same', activation='relu', \n                                   kernel_initializer=\"he_normal\")(drop2)\n    conv3 = residual_block(conv3,start_f)\n    conv3 = residual_block(conv3,start_f, True)                                                         \n    pool3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv3) #32_32_64\n    drop3 = tf.keras.layers.Dropout(0.5)(pool3)\n    start_f = start_f*2\n    conv4 = tf.keras.layers.Conv2D(filters=start_f,  # 32_32_128\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same', activation='relu', \n                                   kernel_initializer=\"he_normal\")(drop3)\n    conv4 = residual_block(conv4,start_f)\n    conv4 = residual_block(conv4,start_f, True) \n    pool4 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv4) # 16_16_128\n    drop4 = tf.keras.layers.Dropout(0.5)(pool4)\n\n    start_f = start_f*2\n    conv5 = tf.keras.layers.Conv2D(filters=start_f,  # 16_16_256\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same', activation='relu', \n                                         kernel_initializer=\"he_normal\")(drop4)\n    \n    conv5 = residual_block(conv5,start_f)\n    conv5 = residual_block(conv5,start_f, True)\n    # Decoder + skip connection\n    # -------\n    \n    up1 = tf.keras.layers.Conv2DTranspose(filters = start_f, #32_32_256\n                                          kernel_size=(3, 3),\n                                          strides=(2, 2),\n                                          padding='same')(conv5)\n    concat1 = tf.keras.layers.Concatenate()\n    merge1 = concat1([up1,conv4])\n    drop5 = tf.keras.layers.Dropout(0.5)(merge1)\n    start_f = start_f // 2\n    convt1 = tf.keras.layers.Conv2D(filters=start_f, #32_32_128\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',activation='relu', \n                                    kernel_initializer=\"he_normal\")(drop5)\n    convt1 = residual_block(convt1,start_f)\n    convt1 = residual_block(convt1,start_f, True)                                 \n    up2 = tf.keras.layers.Conv2DTranspose(filters = start_f, #64_64_128 \n                                          kernel_size=(3, 3),\n                                          strides=(2, 2),\n                                          padding='same', kernel_initializer=\"he_normal\")(convt1)\n    concat2 = tf.keras.layers.Concatenate()\n    merge2 = concat2([up2,conv3])\n    drop6 = tf.keras.layers.Dropout(0.5)(merge2)\n    start_f = start_f // 2\n    convt2 = tf.keras.layers.Conv2D(filters=start_f, #64_64_64\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',activation='relu',\n                                    kernel_initializer=\"he_normal\")(drop6)\n    convt2 = residual_block(convt2,start_f)\n    convt2 = residual_block(convt2,start_f, True)                            \n    \n    up3 = tf.keras.layers.Conv2DTranspose(filters = start_f, #128_128_64\n                                          kernel_size=(3, 3),\n                                          strides=(2, 2),\n                                          padding='same', kernel_initializer=\"he_normal\")(convt2)\n    concat3 = tf.keras.layers.Concatenate()\n    merge3 = concat3([up3, conv2])\n    drop7 = tf.keras.layers.Dropout(0.5)(merge3)\n    start_f = start_f // 2                                      \n    convt3 = tf.keras.layers.Conv2D(filters=start_f, #128_128_32\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',activation='relu',\n                                    kernel_initializer=\"he_normal\")(drop7)\n    convt3 = residual_block(convt3,start_f)\n    convt3 = residual_block(convt3,start_f, True)    \n    \n    up4 = tf.keras.layers.Conv2DTranspose(filters = start_f, #256_256_32\n                                          kernel_size=(3, 3),\n                                          strides=(2, 2),\n                                          padding='same', kernel_initializer=\"he_normal\")(convt3)\n    concat4 = tf.keras.layers.Concatenate()\n    merge4 = concat4([up4,conv1])\n    drop8 = tf.keras.layers.Dropout(0.5*0.05)(merge4)\n    start_f = start_f // 2                                     \n    convt4 = tf.keras.layers.Conv2D(filters=start_f, #256_256_16\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',activation='relu',\n                                    kernel_initializer=\"he_normal\")(drop8)\n    convt4 = residual_block(convt4,start_f)\n    convt4 = residual_block(convt4,start_f, True) \n    \n    # Prediction Layer\n    # ----------------\n    last = tf.keras.layers.Conv2D(filters=1, #256_256_2\n                                     kernel_size=(1, 1),\n                                     strides=(1, 1),\n                                     padding='same',\n                                     activation='sigmoid')(convt4)\n    model = tf.keras.Model(inputs=inputs, outputs=last)\n    \n    return model","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## run following cell only if you want to see the model"},{"metadata":{"id":"u-QCvJ8vega5","colab_type":"code","outputId":"c0511af7-957a-4ef4-dcea-05e6a8968dc5","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"img_w=256\nimg_h=256\nmodel = create_model_noaug(32)\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=60)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dice loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def soft_dice_loss(y_true, y_pred, epsilon=1e-6): # fixed epsilon\n    y_pred = tf.cast(y_pred , tf.float32)\n    y_true = tf.cast(y_true , tf.float32)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    \n    return 1 - tf.reduce_sum(numerator / (denominator + epsilon)) # average over classes and batch","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# optimizer learning rate and evaluation metric"},{"metadata":{"id":"yjzBsEXXegbE","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"lr = 1e-3\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n# -------------------\n\n# Validation metrics\n# ------------------\ndef my_IoU(y_true, y_pred):\n    # from pobability to predicted class {0, 1}\n    y_pred = tf.cast(y_pred > 0.5, tf.float32) # when using sigmoid. Use argmax for softmax\n\n    # A and B\n    intersection = tf.reduce_sum(y_true * y_pred)\n    # A or B\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    # IoU\n    return intersection / union","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load_weights:\n## link to checkpoints of best performing iterations:\n* ### https://polimi365-my.sharepoint.com/:f:/g/personal/10652220_polimi_it/Ery2R-9B0p5IpBn7eMuH40IBgao5Wxh6_NxTTwuU7NSl-w?e=QT79K5\n### make sure you downloaded the checkpoints and put the folders in cwd"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------------------------\nrestore_model = True\nif restore_model:\n    model_noaug7300 = create_model_noaug(32)\n    model_noaug7292 = create_model_noaug(32)\n    model_noaug7330 = create_model_noaug(32)\n    model_noaug7303 = create_model_noaug(32)\n    model_noaug7370= create_model_noaug(32)\n    model_noaug7317= create_model_noaug(32)\n\n    model_noaug7300.compile(loss=soft_dice_loss, \n                  metrics=[my_IoU])  \n    model_noaug7292.compile(loss=soft_dice_loss, \n                  metrics=[my_IoU])\n    model_noaug7330.compile(loss=soft_dice_loss, \n                  metrics=[my_IoU])\n    model_noaug7303.compile(loss=soft_dice_loss, \n                  metrics=[my_IoU])\n    model_noaug7300.load_weights(os.path.join(\n        cwd,'_7300', 'cp_03.ckpt'))  \n    model_noaug7292.load_weights(os.path.join(\n        cwd, '_7292', 'cp_03.ckpt')) \n    model_noaug7330.load_weights(os.path.join(\n        cwd, '_7330', 'cp_03.ckpt')) \n    model_noaug7303.load_weights(os.path.join(\n        cwd, '_7303', 'cp_02.ckpt')) \n    model_noaug7370.load_weights(os.path.join(\n        cwd, '_7370', 'cp_03.ckpt'))\n    model_noaug7317.load_weights(os.path.join(\n        cwd, '_7317', 'cp_04.ckpt'))","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6fbf7240>"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6f89bf28>"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6f715fd0>"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6f8370f0>"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6f6f1080>"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e6f9f7a90>"},"metadata":{}}]},{"metadata":{"id":"I4T2R2CXavtm","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def load(filename):\n   np_image = Image.open(filename)\n   np_image = np.array(np_image).astype('float32')/255\n   np_image = np.expand_dims(np_image, axis=0)\n   return np_image","execution_count":37,"outputs":[]},{"metadata":{"id":"f6JApbsczzrN","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def rle_encode(img):\n      # Flatten column-wise\n      pixels = img.T.flatten()\n      pixels = np.concatenate([[0], pixels, [0]])\n      runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n      runs[1::2] -= runs[::2]\n      return ' '.join(str(x) for x in runs)","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sum of all the IoU scores of the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = 2.9225","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"id":"s1MAsyK2Mz2w","colab_type":"code","outputId":"e0107917-1eb3-41c8-9881-3d90ce343941","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["85703008ac424cbd9535c46e6c0ed84c","9102c3ae05e44e86aade8ea47e6fd3c4","34bcb477e5eb46c4ae8ced18ddd28b1e","8a4720f3063743c9b0b15cccb22e5b84","72d780d57fb34669ae018fbf3f7b9d7c","81d091dba260408db1c6ac3baa698267","aa5f8187f6754785a00768a5d9e44b50","b0aa897534874c5b8fd0c506965d15db"]},"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\ntest_images = {}\nfor i in tqdm(os.listdir(os.path.join(test_dir,'images','img'))):\n    image = load(os.path.join(test_dir,'images','img',i))\n \n    prediction_noaug7300 = model_noaug7300.predict(image)\n    prediction_noaug7292 = model_noaug7292.predict(image)\n    prediction_noaug7330 = model_noaug7330.predict(image)\n    prediction_noaug7303 = model_noaug7303.predict(image)\n    prediction_noaug7370 = model_noaug7370.predict(image)\n    prediction_noaug7317 = model_noaug7317.predict(image)\n\n    prediction_noaug7300 = prediction_noaug7300[0]\n    prediction_noaug7292 = prediction_noaug7292[0]\n    prediction_noaug7330 = prediction_noaug7330[0]\n    prediction_noaug7303 = prediction_noaug7303[0]\n    prediction_noaug7370 = prediction_noaug7370[0]\n    prediction_noaug7317 = prediction_noaug7317[0]\n\n    prediction_img = np.zeros([image.shape[1],image.shape[2],3])\n    prediction_img[np.where(tf.math.add_n([prediction_noaug7292*(0.7292/tot),prediction_noaug7370*(0.7370/tot),prediction_noaug7317*(0.7317/tot),prediction_noaug7300*(0.7300/tot),prediction_noaug7303*(0.7303/tot), prediction_noaug7330*(0.7330/tot)]) < 0.5)] = colors_dict[0]\n    prediction_img[np.where(tf.math.add_n([prediction_noaug7292*(0.7292/tot),prediction_noaug7370*(0.7370/tot),prediction_noaug7317*(0.7317/tot),prediction_noaug7300*(0.7300/tot),prediction_noaug7303*(0.7303/tot), prediction_noaug7330*(0.7330/tot)]) > 0.5)] = colors_dict[1]\n\n    rl = rle_encode(prediction_img)\n    test_images[i[:-4]] = rl","execution_count":79,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e466bb9e284b46a6bc5ebac4468ba6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"id":"tcT1OCUra1D1","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import os\nfrom datetime import datetime\n\ndef create_csv(results, results_dir='./'):\n\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(csv_fname, 'w') as f:\n\n      f.write('ImageId,EncodedPixels,Width,Height\\n')\n\n      for key, value in results.items():\n          f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')\n","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"id":"2pmOzldLAEIy","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"create_csv(test_images)","execution_count":80,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"CNN_Segmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"85703008ac424cbd9535c46e6c0ed84c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9102c3ae05e44e86aade8ea47e6fd3c4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_34bcb477e5eb46c4ae8ced18ddd28b1e","IPY_MODEL_8a4720f3063743c9b0b15cccb22e5b84"]}},"9102c3ae05e44e86aade8ea47e6fd3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34bcb477e5eb46c4ae8ced18ddd28b1e":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_72d780d57fb34669ae018fbf3f7b9d7c","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1234,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1234,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_81d091dba260408db1c6ac3baa698267"}},"8a4720f3063743c9b0b15cccb22e5b84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa5f8187f6754785a00768a5d9e44b50","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 1234/1234 [11:42&lt;00:00,  1.83it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0aa897534874c5b8fd0c506965d15db"}},"72d780d57fb34669ae018fbf3f7b9d7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"81d091dba260408db1c6ac3baa698267":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa5f8187f6754785a00768a5d9e44b50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b0aa897534874c5b8fd0c506965d15db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":1}